
# ðŸŽ™ï¸ Build a Live Voice Chatbot with Agora ConvoAI and GPT-4

![header image](/public/images/convoai-demo-header.png)

## Intro ##

Thereâ€™s something magical about gluing together real-time systems and watching the feedback loop play out in real time. That magic is what drew me to Agoraâ€™s **ConvoAI**â€”a framework that bridges speech, LLM reasoning, and TTS playback in a single pipeline.  

This wasnâ€™t a formal build or a hackathon sprint. It started with a curiosity: _â€œCan I wire up my mic, speak naturally, and get a GPT-4-powered bot to talk back to me using Agora?â€_  

The answer is yesâ€”but it took a few iterations, some console spelunking, and help from a few LLM nudges along the way. This tutorial captures the journey: the wrong starts, the modular wins, and how I used LLM's as both a compass and a cleanup tool.

Furthermore, weâ€™ll walk through how to use [Agoraâ€™s ConvoAI](https://www.agora.io/en/conversational-ai/) to build a live, voice-based chatbot powered by GPT-4. By combining Agoraâ€™s real-time audio SDK with their Conversational AI Engine, we can create a seamless interaction loop where a user speaks into a microphone, the speech is transcribed, processed by a large language model, and the response is spoken back in real time.

Weâ€™ll go from initial setup to streaming conversations, highlighting how LLMs can help throughout developmentâ€”from designing architecture to cleaning up the final user interface.

## Prerequisites ##

Before starting, you should be familiar with:

- Basic JavaScript/TypeScript and React
- REST APIs and environment variables
- Using npm/yarn and setting up a Node.js project
- Access to the Agora Console and an Agora developer account

Youâ€™ll also need:
- An Agora project with RTC and ConvoAI enabled
- An OpenAI API Key (or another LLM provider supported by ConvoAI)

ðŸ”— https://www.agora.io/en/blog/how-to-get-started-with-agora?utm_source=medium&utm_medium=blog&utm_campaign=Build_a_Live_Voice_Chatbot_with_Agora_ConvoAI_and_GPT-4

## Project Setup ##

Weâ€™ll use a Next.js app with API routes and client components to wire everything together.

```bash
npx create-next-app@latest agora-convoai-demo
cd agora-convoai-demo
npm install agora-rtc-sdk-ng dotenv
```

Create a `.env.local` file:

```env
NEXT_PUBLIC_AGORA_APP_ID=your_agora_app_id
AGORA_APP_CERTIFICATE=your_agora_cert
NEXT_PUBLIC_DEFAULT_CHANNEL=demo
OPENAI_API_KEY=your_openai_key
CONVOAI_CLIENT_ID=your_convoai_client_id
CONVOAI_CLIENT_SECRET=your_convoai_client_secret
```

## Build Voice Channel ##

### Join RTC Channel with Mic ###

```tsx
// components/RtcClient.tsx
import { useEffect, useRef, useState } from "react";
import AgoraRTC from "agora-rtc-sdk-ng";

const APP_ID = process.env.NEXT_PUBLIC_AGORA_APP_ID!;
const CHANNEL = process.env.NEXT_PUBLIC_DEFAULT_CHANNEL!;
const UID = Math.floor(Math.random() * 100000);

export default function RtcClient({ token }: { token: string }) {
  const [joined, setJoined] = useState(false);
  const clientRef = useRef(AgoraRTC.createClient({ mode: "rtc", codec: "vp8" }));

  useEffect(() => {
    if (!joined) return;
    const join = async () => {
      await clientRef.current.join(APP_ID, CHANNEL, token, UID);
      const micTrack = await AgoraRTC.createMicrophoneAudioTrack();
      await clientRef.current.publish([micTrack]);
    };
    join();
  }, [joined]);

  return (
    <button onClick={() => setJoined(true)}>Join Channel</button>
  );
}
```

## Build ConvoAI Agent ##

### Backend Start/Stop Agent ###

```ts
// pages/api/agent/start.ts
export default async function handler(req, res) {
  const payload = {
    channel: "demo",
    user_id: "bot",
    llm: {
      provider: "openai",
      model: "gpt-4",
      api_key: process.env.OPENAI_API_KEY
    },
    voice: "en-US-JennyNeural"
  };

  const result = await fetch("https://api.agora.io/conversationalai/v1/project/YOUR_PROJECT_ID/agent/start", {
    method: "POST",
    headers: {
      Authorization: "Basic " + Buffer.from(`${process.env.CONVOAI_CLIENT_ID}:${process.env.CONVOAI_CLIENT_SECRET}`).toString("base64"),
      "Content-Type": "application/json"
    },
    body: JSON.stringify(payload)
  });

  const data = await result.json();
  res.status(200).json(data);
}
```

## Testing Voice Chatbot ##

1. Join the RTC channel.
2. Use your mic to speak.
3. The bot agent should join and speak back a response generated by GPT-4.

Monitor agent join logs in the Agora Console and watch for HTTP 200 responses from the API.

## Conclusion ##

Weâ€™ve successfully created a real-time voice chatbot using Agora ConvoAI and GPT-4. This framework offers flexibility for adding captioning, user interfaces, or multi-agent setups.

ðŸ”— GitHub Repo: https://github.com/your-username/agora-convoai-demo

## Other Resources ##

- [Agora ConvoAI Documentation](https://docs.agora.io/en/conversational-ai/)
- [Agora RTC SDK](https://docs.agora.io/en/voice/)
- [Agora GitHub Examples](https://github.com/AgoraIO)
- I also invite you to join the Agora.io Developer Slack community: [Join Here](https://www.agora.io/en/join-slack/

